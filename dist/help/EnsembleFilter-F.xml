<method>

	<name>Ensemble Filter</name>

	<reference>
		<ref>C. E. Brodley and M. A. Friedl, “Identifying Mislabeled Training Data”, Journal of Artificial Intelligence Research, vol. 11, pp. 131–167, 1999.</ref>
	</reference>

	<generalDescription>  

		<type>Noisy Data Filter</type>

		<objective>To remove noisy instances from the training data.</objective>

		<howWork>
Ensemble Filter (EF) is a well-known filter in the literature. It pretends to improve the quality
of the training data as a preprocessing step in classification, by detecting and eliminating mislabeled
examples. It uses a set of learning algorithms to create classifiers on several subsets of the training
data that serve as noise filters for the training set. The identification of potentially noisy instances
is made by performing an n-fold cross-validation over the training data with m classification algorithms,
called filter algorithms. The complete process carried out by EF is described below:
			
1) Split the training data into n equal sized parts.
2) For each of these n parts, the m filters algorithms are trained on the other n − 1 parts. 
3) The m resulting classifiers are then used to tag each instance in the excluded part as either correct or
mislabeled, by comparing the training label with the assigned by the classifier.
4) At the end of the above process, each instance in the training data has been tagged by each filter
algorithm. This information is used to decide which instances must be removed from the training set by means
a vote-based scheme. 

There are two possible schemes to determine which instances remove: consensus vote or majority vote. Consensus
scheme removes an example if it is misclassified by all the classifiers, while majority scheme removes an
instance if it is misclassified by more than half of the classifiers. Consensus filters are characterized for
to be conservative at throwing away good data at the expense of retaining bad data. Majority filters are better
at detecting bad data at the expense of throwing away good data.
		</howWork>

		<parameterSpec>
			
			<param>numPartitions: number of partitions of the training set.
			</param>
			
			<param>filterType: consensus or majority.
			</param>
			
			<param>numNeighbors: for K-NN algorithm, the number of neighbors.
			</param>
			
			<param>distanceType: for K-NN algorithm, the distance type.
			</param>
			
			<param>confidence: confidence for C4.5 algorithm.
			</param>

			<param>itemsetsPerLeaf: minimum of itemsets per leaf for C4.5 algorithm.
			</param>

			
		</parameterSpec>

		<properties>

			<continuous>Yes</continuous>

			<discretized>Yes</discretized>

			<integer>Yes</integer>

			<nominal>Yes</nominal>

			<valueLess>No</valueLess>

			<impreciseValue>No</impreciseValue>

		</properties>

	</generalDescription>
	
	<example>
		
	</example>

</method>